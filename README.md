# Text Summarisation Project

---

## Overview

This repository contains the final project for text summarisation using deep learning sequence-to-sequence models. We explore three architectures—RNN, LSTM, and GRU—trained and evaluated on the [BillSum](https://huggingface.co/datasets/FiscalNote/billsum) dataset of U.S. Congressional and California state bills. The goal is to generate concise summaries of legislative texts and compare model performance using ROUGE metrics.

## Features

* **Data Preprocessing**: Tokenization, padding, and truncation using Hugging Face `AutoTokenizer`.
* **Model Implementations**:

  * Simple RNN Seq2Seq
  * LSTM Seq2Seq
  * GRU Seq2Seq
* **Evaluation**: Automated evaluation with ROUGE-1, ROUGE-2, and ROUGE-L metrics.
* **Experiment Tracking**: Integrated with Neptune.ai for logging training runs.

## Installation

**Clone the repository**

   ```bash
   git clone https://github.com/YourUsername/text-summarisation
   cd text-summarisation
   ```
**Launch Jupyter Notebooks**

   ```bash
   jupyter notebook
   ```

   * Open `Text_Summarisation_Notebook.ipynb` for the end-to-end pipeline.

## Results

| Model   | ROUGE-1  | ROUGE-2  | ROUGE-L  |
| ------- | -------- | -------- | -------- |
| RNN     | 0.93     | 0.88     | 0.92     |
| LSTM    | 0.94     | 0.90     | 0.93     |
| **GRU** | **0.95** | **0.92** | **0.95** |

The GRU-based Seq2Seq model achieved the highest performance across all three ROUGE metrics.

## Tools & Technologies

* Python 3.8+
* Hugging Face Transformers & Datasets
* TensorFlow & PyTorch (for model comparison)
* ROUGE-Score for evaluation
* Neptune.ai for experiment tracking

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

*Happy summarising!*
